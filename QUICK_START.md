# ğŸš€ Quick Start - Production Training

## âš¡ Cháº¡y nhanh nháº¥t (Recommended)

```bash
# 1. Äi Ä‘áº¿n project directory
cd /home/quanpv/project/traffic_buddy

# 2. Activate virtual environment (náº¿u cÃ³)
source .venv/bin/activate

# 3. Cháº¡y training production
bash scripts/train_production.sh
```

**Done!** Training sáº½ tá»± Ä‘á»™ng:
- âœ… Build dataset náº¿u chÆ°a cÃ³
- âœ… TÃ¬m hyperparameters tá»‘t nháº¥t vá»›i Ray Tune
- âœ… Save checkpoints vÃ o `checkpoints/lora_sft/`
- âœ… Log metrics vÃ o WandB
- âœ… Save best model cuá»‘i cÃ¹ng

---

## ğŸ›ï¸ Custom Configuration

Muá»‘n thay Ä‘á»•i cáº¥u hÃ¬nh? Set environment variables:

```bash
# DÃ¹ng model nhá» hÆ¡n (nhanh hÆ¡n, Ã­t RAM hÆ¡n)
BASE_MODEL="Qwen/Qwen3-VL-2B-Instruct" bash scripts/train_production.sh

# Cháº¡y nhiá»u trials hÆ¡n
NUM_SAMPLES=20 bash scripts/train_production.sh

# Äá»•i output directory
OUTPUT_DIR="./checkpoints/my_training" bash scripts/train_production.sh
```

---

## ğŸ“Š Monitor Training

### Option 1: WandB Dashboard (Best)
Tá»± Ä‘á»™ng log vÃ o WandB, má»Ÿ browser xem real-time:
```
https://wandb.ai/your-username/zalo_challenge
```

### Option 2: Check Log Files
```bash
# Xem log gáº§n nháº¥t
tail -f outputs/logs/training_*.log

# Hoáº·c list all logs
ls -lth outputs/logs/
```

### Option 3: GPU Monitoring
```bash
watch -n 5 nvidia-smi
```

---

## ğŸ“ Káº¿t quáº£ Training

### Best Model Location
```bash
# TÃ¬m best model
find checkpoints/lora_sft -name "best" -type d
```

### Model Structure
```
checkpoints/lora_sft/
â””â”€â”€ trial_lr0.0002_r128/
    â”œâ”€â”€ best/              # â­ Model tá»‘t nháº¥t (eval_loss tháº¥p nháº¥t)
    â”œâ”€â”€ final/             # Model cuá»‘i cÃ¹ng
    â””â”€â”€ checkpoint-*/      # Intermediate checkpoints
```

---

## ğŸ” Check Training Status

### Quick Check
```bash
# Xem GPU usage
nvidia-smi

# Xem log file
tail -20 outputs/logs/training_*.log

# Check checkpoints Ä‘Ã£ táº¡o
ls checkpoints/lora_sft/
```

---

## â±ï¸ Thá»i gian Training

- **Single Trial**: 2-4 hours
- **8 Trials (default)**: 16-32 hours
- **20 Trials**: 40-80 hours

*Thá»i gian thá»±c táº¿ phá»¥ thuá»™c vÃ o GPU vÃ  dataset size*

---

## ğŸ¯ Next Steps

Sau khi training xong:

1. **Evaluate model**:
```bash
python src/evaluation/eval_vllm.py
```

2. **Submit result**:
```bash
# Generate submission file
python src/evaluation/eval_vllm.py
```

3. **Compare models**:
```bash
# Xem metrics trong WandB dashboard
```

---

## â“ Gáº·p váº¥n Ä‘á»?

1. **Out of Memory?** 
   - Giáº£m batch size trong code
   - Giáº£m num_frames

2. **Training cháº­m?**
   - Giáº£m NUM_SAMPLES
   - Táº¯t early stopping cho thá»­ nghiá»‡m

3. **KhÃ´ng cÃ³ GPU?**
   - DÃ¹ng train_simple.py vá»›i CPU
   - Hoáº·c dÃ¹ng Google Colab

---

## ğŸ“š Chi tiáº¿t hÆ¡n?

Xem [TRAINING_GUIDE.md](TRAINING_GUIDE.md) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t vá»:
- Hyperparameter tuning
- Custom configurations  
- Advanced options
- Troubleshooting

